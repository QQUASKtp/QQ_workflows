{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select-For Structure from Motion (SfM) workflow\n",
    "----------------------\n",
    "\n",
    "The following workflow demonstrates the full SfM process using open source libraries on a recently collected over a a forest plantation managed by Select-For. The aim of the collecting and processing the data is to provide a DSM, classified point cloud and Canopy Height Model (CHM). SfM is not really optimal for CHM generation, but can  give a good enough model with a combination of automated and manual routines. \n",
    "\n",
    "This dataset consists of 150 RGB image captures using the SONY A-6000 sensor aboard the Bramor PPX fixed wing platform.\n",
    "\n",
    "This is a subset of the original dataset (~5000 images) as processing would be too lengthy for the demonstration. \n",
    "\n",
    "Survey conditions were good with diffuse light for the duration of the flight.\n",
    "\n",
    "GPS is corrected using the Ordnance survey network.\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "You will need to install this python module to run bash commands without constant exclamation marks\n",
    "\n",
    "pip install bash_kernel\n",
    "\n",
    "python -m bash_kernel.install\n",
    "\n",
    "Firstly install the MicMac lib.\n",
    "\n",
    "https://micmac.ensg.eu/index.php/Accueil\n",
    "\n",
    "The SfM scripts are here and must be added to your system path to work.\n",
    "\n",
    "https://github.com/Ciaran1981/Sfm\n",
    "\n",
    "\n",
    "QGIS 3 offers a versatile platform for this purpose for the final datasets, so please ensure this is installed. Once installed, install the HCMGIS plugin to use various web-based base-layers for visualisation purposes. \n",
    "\n",
    "https://www.osgeo.org/projects/qgis/\n",
    "\n",
    "Cloud compare or meshlab provide a useful tool to visualise point cloud results and can be found through the links below.\n",
    "\n",
    "https://www.danielgm.net/cc/\n",
    "\n",
    "http://www.meshlab.net/\n",
    "\n",
    "Part of the final section of this workflow uses PDAL to provide an initial classification. It is likely best to install this in a separate anaconda environment.  \n",
    "\n",
    "https://pdal.io/\n",
    "\n",
    "Installation of pdal is most easily achieved via anaconda, so remember to change kernel at the appropriate point.\n",
    "\n",
    "**Assumed knowledge:**\n",
    "\n",
    "Basic GIS, image processing, remote sensing and command line use\n",
    "\n",
    "Remember to query function args open a new cell (the plus icon) and write:\n",
    "\n",
    "```bash\n",
    "Orientation.sh -help\n",
    "```\n",
    "**Though each of these commands have a check list and a proceed question first**\n",
    "\n",
    "The data is temporarily found in this google drive folder until QinetiQ find somewhere to store it.\n",
    "\n",
    "https://drive.google.com/drive/folders/1CLjXTn5iT4JuGI-LrB1aErKh9I-X8-VG?usp=sharing\n",
    "\n",
    "The file required is **Forest.zip** - extract this somewhere appropriate and change dir to this folder.\n",
    "\n",
    "This contains: \n",
    "\n",
    "- a collection of JPG images \n",
    "- plot2.csv (the main GPS log)\n",
    "- calib.csv (a lens caslibration subset - optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert you working directory\n",
    "cd /path/wrkdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command Breakdown\n",
    "\n",
    "It is usually best practice to evaluate bundle adjustment results etc., hence processing individually will aid in checking, the sparse point cloud from which the eventual dense one is derived.\n",
    "\n",
    "For this demo, the rgb_sfm using the PIMs (per-image-matching) and forest parameters will suffice.\n",
    "\n",
    "The workflow could be split up into individual commands for each stage also. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Orienation\n",
    "\n",
    "Here we detect features and orient images. \n",
    "\n",
    "As the dataset GPS log has been base-station corrected, we can set an estimation (conservative still!) of 1 meter. \n",
    "\n",
    "At the end the sparse point cloud will open in meshlab (remember to install this!) for evaluation).\n",
    "\n",
    "I have commented out the calibration subset as this not manadatory but illustrates the underlying process and may be a good option depending on time/dataset size constraints. Otherwize the MicMac Martini command is used to intialise the image block without radial distortion, which is then fed to the global orientation. \n",
    "\n",
    "The salient commands internal to the script are as follows, where input variables are prefixed with $ (I have excluded conditional statements and admin cmds to make this clear- look at the script for complete detail):\n",
    "\n",
    "```bash\n",
    "# Generate a GPS and photo index\n",
    "mm3d OriConvert OriTxtInFile ${CSV} RAWGNSS_N ChSys=DegreeWGS84@SysUTM.xml MTD1=1 NameCple=FileImagesNeighbour.xml CalcV=1 \n",
    "\n",
    "# SIFT-based feature detection and matching\n",
    "mm3d Tapioca File FileImagesNeighbour.xml -1  @SFS\n",
    "\n",
    "# Tie point reduction to an evenly spaced set\n",
    "mm3d Schnaps .*${EXTENSION} MoveBadImgs=1\n",
    "\n",
    "# Initialise orientation without lense distortion\n",
    "mm3d Martini .*${EXTENSION}\n",
    "\n",
    "# Relative image orientation\n",
    "mm3d Tapas ${CALIB} .*${EXTENSION} Out=Arbitrary inOri=Martini | tee ${CALIB}RelBundle.txt\n",
    "\n",
    "# Change coordinate system for GPS aided bundle adjustment\n",
    "mm3d CenterBascule .*${EXTENSION} Arbitrary RAWGNSS_N Ground_Init_RTL\n",
    "\n",
    "#Bundle adjust using both camera positions and tie points (number in EmGPS option is the quality estimate of the GNSS data in meters)\n",
    "mm3d Campari .*${EXTENSION} Ground_Init_RTL Ground_UTM EmGPS=[RAWGNSS_N,1] AllFree=1  | tee ${CALIB}GnssBundle.txt\n",
    "\n",
    "#Generate a sparse pointcloud for inspection\n",
    "mm3d AperiCloud .*${EXTENSION} Ground_UTM\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or include the calib-subset\n",
    "# uncomment the line to run\n",
    "# Orientation.sh -e tif -u \"30 +north\" -c Fraser -t \"plot2.csv\" -s \"sub.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Orientation.sh -e tif -u \"30 +north\" -c Fraser -t \"plot2.csv\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense matching & ortho-photos\n",
    "\n",
    "For this dataset the PIMs and forest modes are used and given the diffuse lighting no radiometric equalisation will be carried out.\n",
    "\n",
    "\n",
    "The main process is carried out via the bash script below, which itself contains the following sequnce of MicMac commands which are critical to the outputs being produced. **Some other things are done to move files around so please look at the script for details of everything**. Input variables are prefixed with $. \n",
    "\n",
    "```bash\n",
    "\n",
    "#Calculate dense cloud\n",
    "mm3d PIMs $Algorithm .*$EXTENSION Ground_UTM DefCor=0 ZoomF=$ZoomF ZReg=$zreg Masq3D=AperiCloud_Ground_UTM.ply.xml\n",
    "\n",
    "#Generate DSM and ortho photos\n",
    "mm3d PIMs2MNT $Algorithm DoMnt=1 DoOrtho=1\n",
    "\n",
    "#Generate the Mosaic\n",
    "mm3d Tawny PIMs-ORTHO/ RadiomEgal=0 Out=Orthophotomosaic.tif\n",
    "\n",
    "#Merge the mosaic tiles if they exist \n",
    "mm3d ConvertIm PIMs-ORTHO/Orthophotomosaic.tif Out=OUTPUT/OrthFinal.tif\n",
    "cp PIMs-ORTHO/Orthophotomosaic.tfw OUTPUT/OrthFinal.tfw\n",
    "\n",
    "#Georeference the image (same is done for DSM)\n",
    "gdal_edit.py -a_srs \"+proj=utm +zone=$UTM +ellps=WGS84 +datum=WGS84 +units=m +no_defs\" OUTPUT/OrthFinal.tif\n",
    "\n",
    "#Generate point cloud\n",
    "mm3d Nuage2Ply PIMs-TmpBasc/PIMs-Merged.xml Attr=PIMs-ORTHO/Orthophotomosaic.tif Out=OUTPUT/pointcloud.ply\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_cloud.sh -e tif -a Forest -m PIMs -u 30 +north -i 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs\n",
    "\n",
    "We can view the results in QGIS for the imagery and meshlab/cloudcompare for the point cloud "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mm3d Nuage2Ply PIMs-TmpBasc/PIMs-Merged.xml Attr=PIMs-ORTHO/Orthophotomosaic.tif Out=OUTPUT/pointcloud.ply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point cloud classification\n",
    "\n",
    "As SfM is modelled approach, it does not lend itself to point cloud classification in the same way LiDAR (a direct measurment of sorts) does.\n",
    "\n",
    "Reasonable results may still be obtained, however with a combination of automated routines and manual editing.\n",
    "\n",
    "For the automated part, the pdal libray is used to provided an initial filtered point cloud from which a manual tidy-up can be performed. \n",
    "\n",
    "**activate pdal first via command line (NOT the notebook)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda activate pdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdal translate -i OUTPUT/psm.ply -o OUTPUT/psm.laz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the initial classification of ground points using the progressive morphological filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdal ground -i OUTPUT/psm.laz -o OUTPUT/classif.laz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we retain only the ground points prior to manual editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdal translate -i classif.laz smrf range --filters.range.limits=\"Classification[2:2]\" -o grnd.laz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual tidying\n",
    "\n",
    "From here we have to manually edit the point cloud using cloud compare, for which there are tutorials online.\n",
    "\n",
    "The procedure follows the process described here:\n",
    "\n",
    "https://www.cloudcompare.org/doc/wiki/index.php?title=Interactive_Segmentation_Tool\n",
    "\n",
    "Through multiple (manual) iterations you can crop the areas you wish to retain/delete until a satisfactory result is obtained. \n",
    "\n",
    "Unfortunately, to obtain better results manual labour is the only way.\n",
    "\n",
    "**Probably best not to embark on this task unless you wish/need to know how to do this manually.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
